{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from transformers import SpeechEncoderDecoderModel, Speech2Text2Processor, Speech2TextProcessor\n",
    "import soundfile as sf\n",
    "\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import librosa\n",
    "import json\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Set, Union\n",
    "\n",
    "import torch\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "wav2vec2_model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-xlsr-53\")\n",
    "wav2vec2_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-large-xlsr-53 were not used when initializing Wav2Vec2ForCTC: ['project_q.bias', 'quantizer.weight_proj.weight', 'project_hid.bias', 'quantizer.codevectors', 'project_hid.weight', 'project_q.weight', 'quantizer.weight_proj.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "class LibriSpeechDataset(Dataset):\n",
    "\tdef __init__(self, json_path):\n",
    "\t\tself.json_path = json_path\n",
    "\t\tself.data = self.load_data_from_json(json_path)\n",
    "\n",
    "\tdef load_data_from_json(self, json_path):\n",
    "\t\twith open(json_path, \"r\") as f:\n",
    "\t\t\tdata = json.load(f)\n",
    "\n",
    "\t\tdata = data[\"data\"]\n",
    "\t\treturn data\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\taudio, _ = librosa.load(self.data[idx][\"file\"], 16000)\n",
    "\t\tinput_value = wav2vec2_processor.feature_extractor(audio, sampling_rate=16000)\n",
    "\t\t# Do some text preprocessing here\n",
    "\t\ttext = self.data[idx][\"text\"]\n",
    "\t\twith wav2vec2_processor.as_target_processor():\n",
    "\t\t\tlabel = wav2vec2_processor(text).input_ids\n",
    "\n",
    "\t\tsample = {\n",
    "\t\t\t\"input_values\": input_value[\"input_values\"][0],\n",
    "\t\t\t\"labels\": label,\n",
    "\t\t\t\"lang\": \"en\",\n",
    "\t\t}\n",
    "\t\treturn sample\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "train_dataset = LibriSpeechDataset(\"/root/develop/KIWI-module/code/wav2byte-pipeline/data/en-librispeech-test-clean-pure-99.0-local-wav.json\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "train_dataset[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_values': array([ 0.00410041,  0.00410041,  0.00681573, ..., -0.0325565 ,\n",
       "        -0.03595066, -0.03323534], dtype=float32),\n",
       " 'labels': [6,\n",
       "  8,\n",
       "  4,\n",
       "  19,\n",
       "  5,\n",
       "  15,\n",
       "  5,\n",
       "  24,\n",
       "  13,\n",
       "  7,\n",
       "  6,\n",
       "  5,\n",
       "  4,\n",
       "  6,\n",
       "  11,\n",
       "  5,\n",
       "  4,\n",
       "  7,\n",
       "  13,\n",
       "  13,\n",
       "  10,\n",
       "  25,\n",
       "  7,\n",
       "  15,\n",
       "  4,\n",
       "  8,\n",
       "  20,\n",
       "  4,\n",
       "  11,\n",
       "  5,\n",
       "  13,\n",
       "  4,\n",
       "  12,\n",
       "  8,\n",
       "  9,\n",
       "  4,\n",
       "  12,\n",
       "  10,\n",
       "  15,\n",
       "  25,\n",
       "  10,\n",
       "  7,\n",
       "  4,\n",
       "  21,\n",
       "  7,\n",
       "  25,\n",
       "  5,\n",
       "  4,\n",
       "  7,\n",
       "  4,\n",
       "  12,\n",
       "  23,\n",
       "  15,\n",
       "  5,\n",
       "  9,\n",
       "  14,\n",
       "  10,\n",
       "  14,\n",
       "  4,\n",
       "  12,\n",
       "  16,\n",
       "  23,\n",
       "  23,\n",
       "  5,\n",
       "  13,\n",
       "  4,\n",
       "  6,\n",
       "  8,\n",
       "  4,\n",
       "  18,\n",
       "  11,\n",
       "  10,\n",
       "  19,\n",
       "  11,\n",
       "  4,\n",
       "  12,\n",
       "  11,\n",
       "  5,\n",
       "  4,\n",
       "  11,\n",
       "  7,\n",
       "  14,\n",
       "  4,\n",
       "  10,\n",
       "  9,\n",
       "  25,\n",
       "  10,\n",
       "  6,\n",
       "  5,\n",
       "  14,\n",
       "  4,\n",
       "  7,\n",
       "  15,\n",
       "  15,\n",
       "  4,\n",
       "  11,\n",
       "  5,\n",
       "  13,\n",
       "  4,\n",
       "  13,\n",
       "  5,\n",
       "  15,\n",
       "  7,\n",
       "  6,\n",
       "  10,\n",
       "  25,\n",
       "  5,\n",
       "  12,\n",
       "  4,\n",
       "  7,\n",
       "  9,\n",
       "  14,\n",
       "  4,\n",
       "  10,\n",
       "  6,\n",
       "  4,\n",
       "  18,\n",
       "  7,\n",
       "  12,\n",
       "  4,\n",
       "  7,\n",
       "  4,\n",
       "  21,\n",
       "  8,\n",
       "  8,\n",
       "  14,\n",
       "  4,\n",
       "  8,\n",
       "  23,\n",
       "  23,\n",
       "  8,\n",
       "  13,\n",
       "  6,\n",
       "  16,\n",
       "  9,\n",
       "  10,\n",
       "  6,\n",
       "  22,\n",
       "  4,\n",
       "  20,\n",
       "  8,\n",
       "  13,\n",
       "  4,\n",
       "  17,\n",
       "  5,\n",
       "  4,\n",
       "  6,\n",
       "  8,\n",
       "  4,\n",
       "  17,\n",
       "  7,\n",
       "  26,\n",
       "  5,\n",
       "  4,\n",
       "  6,\n",
       "  11,\n",
       "  5,\n",
       "  10,\n",
       "  13,\n",
       "  4,\n",
       "  7,\n",
       "  19,\n",
       "  30,\n",
       "  16,\n",
       "  7,\n",
       "  10,\n",
       "  9,\n",
       "  6,\n",
       "  7,\n",
       "  9,\n",
       "  19,\n",
       "  5],\n",
       " 'lang': 'en'}"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "features = [train_dataset[0], train_dataset[1]]\n",
    "\n",
    "\n",
    "\n",
    "input_features = [\n",
    "\t{\"input_values\": feature[\"input_values\"]} for feature in features\n",
    "]\n",
    "label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "batch = wav2vec2_processor.feature_extractor.pad(\n",
    "\tinput_features,\n",
    "\tpadding=True,\n",
    "\tmax_length=1024,\n",
    "\treturn_tensors=\"pt\"\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "batch['input_values'] "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 0.0041,  0.0041,  0.0068,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0098, -0.0153, -0.0098,  ...,  0.0104,  0.0160,  0.0117]])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('pyctc': conda)"
  },
  "interpreter": {
   "hash": "42db0750e4a48b7615e3156eb4bc25d670d6ae94f25d71698afd961b57c83a94"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}